<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0">
  <title>MS Thesis â€“ Aditya Mishra</title>
  <link rel="stylesheet" href="../styles.css">
</head>
<body>
  <nav>
    <div class="nav-inner">
      <div class="brand">Aditya Mishra</div>
      <ul>
        <li><a href="../index.html">About</a></li>
        <li><a href="../research.html" class="active">Research</a></li>
        <li><a href="../projects.html">Projects</a></li>
        <li><a href="../publications.html">Publications</a></li>
        <li><a href="../news.html">News</a></li>
        <li><a href="../contact.html">Contact</a></li>
        <li><a href="../cv.html">CV</a></li>
      </ul>
    </div>
  </nav>

  <main class="container">
    <h2>Nighttime Traffic Sign Recognition</h2>

    <!-- Abstract -->
    <section class="cv-section">
      <h3>Abstract</h3>
      <p style="text-align: justify;">
        Traffic signboards are vital for road safety and intelligent transportation systems, enabling navigation and autonomous driving. Yet, 
        recognizing traffic signs at night remains challenging due to visual noise and scarcity of public nighttime datasets. Despite advances 
        in vision architectures, existing methods struggle with robustness under low illumination and fail to leverage complementary mutlimodal 
        cues effectively. 
      </p>
      <p style="text-align: justify;">
       To overcome these limitations, firstly, we introduce INTSD, a large-scale dataset comprising street-level night-time images 
        of traffic signboards collected across diverse regions of India. The dataset spans 41 traffic signboard classes captured under varying lighting 
        and weather conditions, providing a comprehensive benchmark for both detection and classification tasks. To benchmark INTSD for night-time sign 
        recognition, we conduct extensive evaluations using state-of-the-art detection and classification models. Secondly, we propose LENS-Net, which 
        integrates an adaptive image enhancement detector for joint illumination correction and sign localization, followed by a structured multimodal 
        CLIP-GCNN classifier that leverages cross-modal attention and graph-based reasoning for robust and semantically consistent recognition. 
        Our method surpasses existing frameworks, with ablation studies confirming the effectiveness of its key components. 
        The dataset and code for LENS-Net is publicly available for research.
      </p>
    </section>

    <!-- Acknowledgement -->
    <section class="cv-section">
      <h3>Acknowledgement</h3>
      <p style="text-align: justify;">
        This thesis submitted in partial fulfilment of the requirements for the award of the degree of Master of Science (MS) in Electrical
        Engineering and Computer Science under the supervision of 
        <a href="https://loneharoon.github.io/" style="color: var(--primary);" target="_blank">Dr. Haroon Lone</a>, and
        <a href="https://sites.google.com/iiitd.ac.in/agarwalakshay/home" style="color: var(--primary);" target="_blank">Dr. Akshay Agarwal.</a>
      </p>
    </section>

    <!-- Resources -->
    <section class="cv-section">
      <h3>Resources (To be updated...)</h3>
      <ul>
        <li><a href="" style="color: var(--primary);" target="_blank">View Full Thesis (PDF)</a></li>
        <li><a href="" style="color: var(--primary);" target="_blank">View Presentation (PDF)</a></li>
        <li><a href="" style="color: var(--primary);" target="_blank">GitHub Repository</a></li>
      </ul>
    </section>
  </main>
</body>
</html>
