<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1.0">
  <title>GCVASE â€“ Aditya Mishra</title>
  <link rel="stylesheet" href="../styles.css">
</head>
<body>
  <nav>
    <div class="nav-inner">
      <div class="brand">Aditya Mishra</div>
      <ul>
        <li><a href="../index.html">About</a></li>
        <li><a href="../research.html" class="active">Research</a></li>
        <li><a href="../projects.html">Projects</a></li>
        <li><a href="../publications.html">Publications</a></li>
        <li><a href="../news.html">News</a></li>
        <li><a href="../contact.html">Contact</a></li>
        <li><a href="../cv.html">CV</a></li>
      </ul>
    </div>
  </nav>

  <main class="container">
    <h2>Subject Representation Learning from EEG using Graph Convolutional Autoencoders</h2>

    <!-- Abstract -->
    <section class="cv-section">
      <h3>Abstract</h3>
      <p style="text-align: justify;">
        We propose GC-VASE, a graph convolutional-based variational autoencoder that leverages contrastive learning for subject representation learning from EEG data. 
        Our method successfully learns robust subject-specific latent representations using the split-latent space architecture tailored for subject identification. 
        To enhance the model's adaptability to unseen subjects without extensive retraining, we introduce an attention-based adapter network for fine-tuning, which 
        reduces the computational cost of adapting the model to new subjects. Our method significantly outperforms other deep learning approaches, achieving state-of-the-art 
        results with a subject balanced accuracy of 89.81% on the ERP-Core dataset and 70.85% on the SleepEDFx-20 dataset. After subject 
        adaptive fine-tuning using adapters and attention layers, GC-VASE further improves the subject balanced accuracy to 90.31% on ERP-Core. 
        Additionally, we perform a detailed ablation study to highlight the impact of the key components of our method.
      </p>
    </section>

    <!-- Acknowledgement -->
    <section class="cv-section">
      <h3>Acknowledgement</h3>
      <p style="text-align: justify;">
        I joined the
        <a href="https://www.queensu.ca/" style="color: var(--primary);" target="_blank">Queen's</a>
        School of Computing, Canada to carry out the work under the joint supversion of
        <a href="https://www.aiimlab.com/ali-etemad" style="color: var(--primary);" target="_blank">Dr. Ali Etemad</a>
        and 
        <a href="https://www.cs.queensu.ca/people/Javad/Hashemi" style="color: var(--primary);" target="_blank">Dr. Javad Hashemi</a>
        . This was funded by 
        <a href="https://www.mitacs.ca/our-programs/globalink-research-internship-students/" style="color: var(--primary);" target="_blank">Mitacs Globalink Research Internship (GRI)</a>
        program. 
      </p>
    </section>

    <!-- Resources -->
    <section class="cv-section">
      <h3>Resources</h3>
      <ul>
        <li><a href="https://arxiv.org/abs/2501.16626" style="color: var(--primary);" target="_blank">View Full Publication (PDF)</a></li>
      </ul>
    </section>
  </main>
</body>
</html>
